{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook covers the preprocessing for the cleaned data. The preprocessing will include PCA, and feature selection. It will have various pipelines that can be used to train different sets of processed data to compare the results of a machine learning model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score \n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score\n",
    "import xgboost as xgb\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin preprocessing, we will run some simple models to determine a baseline. This baseline can then be used as a benchmark after completing other preprocessing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>koi_fpflag_nt</th>\n",
       "      <th>koi_fpflag_ss</th>\n",
       "      <th>koi_fpflag_co</th>\n",
       "      <th>koi_fpflag_ec</th>\n",
       "      <th>koi_period</th>\n",
       "      <th>koi_period_err1</th>\n",
       "      <th>koi_period_err2</th>\n",
       "      <th>koi_time0bk</th>\n",
       "      <th>koi_time0bk_err1</th>\n",
       "      <th>koi_time0bk_err2</th>\n",
       "      <th>...</th>\n",
       "      <th>koi_slogg</th>\n",
       "      <th>koi_slogg_err1</th>\n",
       "      <th>koi_slogg_err2</th>\n",
       "      <th>koi_srad</th>\n",
       "      <th>koi_srad_err1</th>\n",
       "      <th>koi_srad_err2</th>\n",
       "      <th>ra</th>\n",
       "      <th>dec</th>\n",
       "      <th>koi_kepmag</th>\n",
       "      <th>koi_disposition_encoded</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rowid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.488036</td>\n",
       "      <td>2.775000e-05</td>\n",
       "      <td>-2.775000e-05</td>\n",
       "      <td>170.538750</td>\n",
       "      <td>0.002160</td>\n",
       "      <td>-0.002160</td>\n",
       "      <td>...</td>\n",
       "      <td>4.467</td>\n",
       "      <td>0.064</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>0.927</td>\n",
       "      <td>0.105</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>291.93423</td>\n",
       "      <td>48.141651</td>\n",
       "      <td>15.347</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>54.418383</td>\n",
       "      <td>2.479000e-04</td>\n",
       "      <td>-2.479000e-04</td>\n",
       "      <td>162.513840</td>\n",
       "      <td>0.003520</td>\n",
       "      <td>-0.003520</td>\n",
       "      <td>...</td>\n",
       "      <td>4.467</td>\n",
       "      <td>0.064</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>0.927</td>\n",
       "      <td>0.105</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>291.93423</td>\n",
       "      <td>48.141651</td>\n",
       "      <td>15.347</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19.899140</td>\n",
       "      <td>1.494000e-05</td>\n",
       "      <td>-1.494000e-05</td>\n",
       "      <td>175.850252</td>\n",
       "      <td>0.000581</td>\n",
       "      <td>-0.000581</td>\n",
       "      <td>...</td>\n",
       "      <td>4.544</td>\n",
       "      <td>0.044</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>0.868</td>\n",
       "      <td>0.233</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>297.00482</td>\n",
       "      <td>48.134129</td>\n",
       "      <td>15.436</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.736952</td>\n",
       "      <td>2.630000e-07</td>\n",
       "      <td>-2.630000e-07</td>\n",
       "      <td>170.307565</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>-0.000115</td>\n",
       "      <td>...</td>\n",
       "      <td>4.564</td>\n",
       "      <td>0.053</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>0.791</td>\n",
       "      <td>0.201</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>285.53461</td>\n",
       "      <td>48.285210</td>\n",
       "      <td>15.597</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.525592</td>\n",
       "      <td>3.761000e-06</td>\n",
       "      <td>-3.761000e-06</td>\n",
       "      <td>171.595550</td>\n",
       "      <td>0.001130</td>\n",
       "      <td>-0.001130</td>\n",
       "      <td>...</td>\n",
       "      <td>4.438</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>1.046</td>\n",
       "      <td>0.334</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>288.75488</td>\n",
       "      <td>48.226200</td>\n",
       "      <td>15.509</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       koi_fpflag_nt  koi_fpflag_ss  koi_fpflag_co  koi_fpflag_ec  koi_period  \\\n",
       "rowid                                                                           \n",
       "1                  0              0              0              0    9.488036   \n",
       "2                  0              0              0              0   54.418383   \n",
       "3                  0              1              0              0   19.899140   \n",
       "4                  0              1              0              0    1.736952   \n",
       "5                  0              0              0              0    2.525592   \n",
       "\n",
       "       koi_period_err1  koi_period_err2  koi_time0bk  koi_time0bk_err1  \\\n",
       "rowid                                                                    \n",
       "1         2.775000e-05    -2.775000e-05   170.538750          0.002160   \n",
       "2         2.479000e-04    -2.479000e-04   162.513840          0.003520   \n",
       "3         1.494000e-05    -1.494000e-05   175.850252          0.000581   \n",
       "4         2.630000e-07    -2.630000e-07   170.307565          0.000115   \n",
       "5         3.761000e-06    -3.761000e-06   171.595550          0.001130   \n",
       "\n",
       "       koi_time0bk_err2  ...  koi_slogg  koi_slogg_err1  koi_slogg_err2  \\\n",
       "rowid                    ...                                              \n",
       "1             -0.002160  ...      4.467           0.064          -0.096   \n",
       "2             -0.003520  ...      4.467           0.064          -0.096   \n",
       "3             -0.000581  ...      4.544           0.044          -0.176   \n",
       "4             -0.000115  ...      4.564           0.053          -0.168   \n",
       "5             -0.001130  ...      4.438           0.070          -0.210   \n",
       "\n",
       "       koi_srad  koi_srad_err1  koi_srad_err2         ra        dec  \\\n",
       "rowid                                                                 \n",
       "1         0.927          0.105         -0.061  291.93423  48.141651   \n",
       "2         0.927          0.105         -0.061  291.93423  48.141651   \n",
       "3         0.868          0.233         -0.078  297.00482  48.134129   \n",
       "4         0.791          0.201         -0.067  285.53461  48.285210   \n",
       "5         1.046          0.334         -0.133  288.75488  48.226200   \n",
       "\n",
       "       koi_kepmag  koi_disposition_encoded  \n",
       "rowid                                       \n",
       "1          15.347                        1  \n",
       "2          15.347                        1  \n",
       "3          15.436                        2  \n",
       "4          15.597                        2  \n",
       "5          15.509                        1  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('..\\data\\kois_cleaned.csv', index_col=0)\n",
    "kois = data.copy()\n",
    "kois.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>koi_fpflag_nt</th>\n",
       "      <th>koi_fpflag_ss</th>\n",
       "      <th>koi_fpflag_co</th>\n",
       "      <th>koi_fpflag_ec</th>\n",
       "      <th>koi_period</th>\n",
       "      <th>koi_time0bk</th>\n",
       "      <th>koi_impact</th>\n",
       "      <th>koi_duration</th>\n",
       "      <th>koi_depth</th>\n",
       "      <th>koi_prad</th>\n",
       "      <th>koi_teq</th>\n",
       "      <th>koi_insol</th>\n",
       "      <th>koi_model_snr</th>\n",
       "      <th>koi_steff</th>\n",
       "      <th>koi_slogg</th>\n",
       "      <th>koi_srad</th>\n",
       "      <th>ra</th>\n",
       "      <th>dec</th>\n",
       "      <th>koi_kepmag</th>\n",
       "      <th>koi_disposition_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9007.000000</td>\n",
       "      <td>9007.000000</td>\n",
       "      <td>9007.000000</td>\n",
       "      <td>9007.000000</td>\n",
       "      <td>9007.000000</td>\n",
       "      <td>9007.000000</td>\n",
       "      <td>9007.000000</td>\n",
       "      <td>9007.000000</td>\n",
       "      <td>9.007000e+03</td>\n",
       "      <td>9007.000000</td>\n",
       "      <td>9007.000000</td>\n",
       "      <td>9.007000e+03</td>\n",
       "      <td>9007.000000</td>\n",
       "      <td>9007.000000</td>\n",
       "      <td>9007.000000</td>\n",
       "      <td>9007.000000</td>\n",
       "      <td>9007.000000</td>\n",
       "      <td>9007.000000</td>\n",
       "      <td>9007.000000</td>\n",
       "      <td>9007.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.167203</td>\n",
       "      <td>0.235039</td>\n",
       "      <td>0.197402</td>\n",
       "      <td>0.122238</td>\n",
       "      <td>75.093189</td>\n",
       "      <td>165.792699</td>\n",
       "      <td>0.722475</td>\n",
       "      <td>5.636540</td>\n",
       "      <td>2.293444e+04</td>\n",
       "      <td>102.880978</td>\n",
       "      <td>1077.611302</td>\n",
       "      <td>7.325554e+03</td>\n",
       "      <td>262.226624</td>\n",
       "      <td>5704.450094</td>\n",
       "      <td>4.310499</td>\n",
       "      <td>1.699245</td>\n",
       "      <td>292.047428</td>\n",
       "      <td>43.828852</td>\n",
       "      <td>14.268715</td>\n",
       "      <td>1.265238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.373178</td>\n",
       "      <td>0.424047</td>\n",
       "      <td>0.398060</td>\n",
       "      <td>0.327579</td>\n",
       "      <td>1375.035264</td>\n",
       "      <td>67.589891</td>\n",
       "      <td>3.296483</td>\n",
       "      <td>6.409275</td>\n",
       "      <td>8.078573e+04</td>\n",
       "      <td>3109.455176</td>\n",
       "      <td>843.714627</td>\n",
       "      <td>1.584377e+05</td>\n",
       "      <td>803.003738</td>\n",
       "      <td>797.670670</td>\n",
       "      <td>0.430133</td>\n",
       "      <td>5.622511</td>\n",
       "      <td>4.760429</td>\n",
       "      <td>3.598752</td>\n",
       "      <td>1.348588</td>\n",
       "      <td>0.821739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.241843</td>\n",
       "      <td>120.515914</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.167000</td>\n",
       "      <td>4.500000e+00</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>2661.000000</td>\n",
       "      <td>0.047000</td>\n",
       "      <td>0.109000</td>\n",
       "      <td>279.852720</td>\n",
       "      <td>36.577381</td>\n",
       "      <td>6.966000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.741232</td>\n",
       "      <td>132.773544</td>\n",
       "      <td>0.196000</td>\n",
       "      <td>2.454245</td>\n",
       "      <td>1.603000e+02</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>537.000000</td>\n",
       "      <td>1.960500e+01</td>\n",
       "      <td>12.100000</td>\n",
       "      <td>5301.000000</td>\n",
       "      <td>4.215000</td>\n",
       "      <td>0.827000</td>\n",
       "      <td>288.660770</td>\n",
       "      <td>40.810164</td>\n",
       "      <td>13.469000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.449426</td>\n",
       "      <td>137.157520</td>\n",
       "      <td>0.534000</td>\n",
       "      <td>3.823530</td>\n",
       "      <td>4.183000e+02</td>\n",
       "      <td>2.380000</td>\n",
       "      <td>875.000000</td>\n",
       "      <td>1.383400e+02</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>5761.000000</td>\n",
       "      <td>4.440000</td>\n",
       "      <td>0.997000</td>\n",
       "      <td>292.249020</td>\n",
       "      <td>43.720329</td>\n",
       "      <td>14.534000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>37.597864</td>\n",
       "      <td>170.545120</td>\n",
       "      <td>0.886000</td>\n",
       "      <td>6.308000</td>\n",
       "      <td>1.413550e+03</td>\n",
       "      <td>14.375000</td>\n",
       "      <td>1372.000000</td>\n",
       "      <td>8.359150e+02</td>\n",
       "      <td>76.650000</td>\n",
       "      <td>6116.000000</td>\n",
       "      <td>4.544000</td>\n",
       "      <td>1.347000</td>\n",
       "      <td>295.856400</td>\n",
       "      <td>46.722311</td>\n",
       "      <td>15.319000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>129995.778400</td>\n",
       "      <td>1472.522306</td>\n",
       "      <td>100.806000</td>\n",
       "      <td>138.540000</td>\n",
       "      <td>1.541400e+06</td>\n",
       "      <td>200346.000000</td>\n",
       "      <td>14667.000000</td>\n",
       "      <td>1.094755e+07</td>\n",
       "      <td>9054.700000</td>\n",
       "      <td>15896.000000</td>\n",
       "      <td>5.364000</td>\n",
       "      <td>180.013000</td>\n",
       "      <td>301.720760</td>\n",
       "      <td>52.336010</td>\n",
       "      <td>19.065000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       koi_fpflag_nt  koi_fpflag_ss  koi_fpflag_co  koi_fpflag_ec  \\\n",
       "count    9007.000000    9007.000000    9007.000000    9007.000000   \n",
       "mean        0.167203       0.235039       0.197402       0.122238   \n",
       "std         0.373178       0.424047       0.398060       0.327579   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "          koi_period  koi_time0bk   koi_impact  koi_duration     koi_depth  \\\n",
       "count    9007.000000  9007.000000  9007.000000   9007.000000  9.007000e+03   \n",
       "mean       75.093189   165.792699     0.722475      5.636540  2.293444e+04   \n",
       "std      1375.035264    67.589891     3.296483      6.409275  8.078573e+04   \n",
       "min         0.241843   120.515914     0.000000      0.167000  4.500000e+00   \n",
       "25%         2.741232   132.773544     0.196000      2.454245  1.603000e+02   \n",
       "50%         9.449426   137.157520     0.534000      3.823530  4.183000e+02   \n",
       "75%        37.597864   170.545120     0.886000      6.308000  1.413550e+03   \n",
       "max    129995.778400  1472.522306   100.806000    138.540000  1.541400e+06   \n",
       "\n",
       "            koi_prad       koi_teq     koi_insol  koi_model_snr     koi_steff  \\\n",
       "count    9007.000000   9007.000000  9.007000e+03    9007.000000   9007.000000   \n",
       "mean      102.880978   1077.611302  7.325554e+03     262.226624   5704.450094   \n",
       "std      3109.455176    843.714627  1.584377e+05     803.003738    797.670670   \n",
       "min         0.140000     25.000000  0.000000e+00       1.600000   2661.000000   \n",
       "25%         1.400000    537.000000  1.960500e+01      12.100000   5301.000000   \n",
       "50%         2.380000    875.000000  1.383400e+02      23.000000   5761.000000   \n",
       "75%        14.375000   1372.000000  8.359150e+02      76.650000   6116.000000   \n",
       "max    200346.000000  14667.000000  1.094755e+07    9054.700000  15896.000000   \n",
       "\n",
       "         koi_slogg     koi_srad           ra          dec   koi_kepmag  \\\n",
       "count  9007.000000  9007.000000  9007.000000  9007.000000  9007.000000   \n",
       "mean      4.310499     1.699245   292.047428    43.828852    14.268715   \n",
       "std       0.430133     5.622511     4.760429     3.598752     1.348588   \n",
       "min       0.047000     0.109000   279.852720    36.577381     6.966000   \n",
       "25%       4.215000     0.827000   288.660770    40.810164    13.469000   \n",
       "50%       4.440000     0.997000   292.249020    43.720329    14.534000   \n",
       "75%       4.544000     1.347000   295.856400    46.722311    15.319000   \n",
       "max       5.364000   180.013000   301.720760    52.336010    19.065000   \n",
       "\n",
       "       koi_disposition_encoded  \n",
       "count              9007.000000  \n",
       "mean                  1.265238  \n",
       "std                   0.821739  \n",
       "min                   0.000000  \n",
       "25%                   1.000000  \n",
       "50%                   2.000000  \n",
       "75%                   2.000000  \n",
       "max                   2.000000  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#exclude error columns with .describe()\n",
    "kois.loc[:, ~kois.columns.str.contains('_err')].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9007, 40)\n"
     ]
    }
   ],
   "source": [
    "print(kois.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate our target variable from the rest of the data\n",
    "y = kois['koi_disposition_encoded']\n",
    "X = kois.drop(['koi_disposition_encoded'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of Total Dispositions for Each Target Variable: \n",
      "koi_disposition_encoded\n",
      "2    0.505385\n",
      "1    0.254469\n",
      "0    0.240147\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Target Variable Descriptive Statistics:\n",
      "count    9007.000000\n",
      "mean        1.265238\n",
      "std         0.821739\n",
      "min         0.000000\n",
      "25%         1.000000\n",
      "50%         2.000000\n",
      "75%         2.000000\n",
      "max         2.000000\n",
      "Name: koi_disposition_encoded, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#print the percentage of values in each class\n",
    "print(f\"Percent of Total Dispositions for Each Target Variable: \\n{y.value_counts(normalize=True)}\")\n",
    "print(f\"\\nTarget Variable Descriptive Statistics:\\n{y.describe()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) #use a test size of 20%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiating Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision tree classifier\n",
    "dtc = DecisionTreeClassifier()\n",
    "\n",
    "#Random forest classifier\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "#Logistic regression classifier\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "#Support vector machine classifier\n",
    "svc = SVC()\n",
    "\n",
    "#K-nearest neighbors classifier\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "#XGBoost classifier\n",
    "xgbc = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-13 {color: black;background-color: white;}#sk-container-id-13 pre{padding: 0;}#sk-container-id-13 div.sk-toggleable {background-color: white;}#sk-container-id-13 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-13 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-13 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-13 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-13 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-13 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-13 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-13 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-13 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-13 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-13 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-13 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-13 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-13 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-13 div.sk-item {position: relative;z-index: 1;}#sk-container-id-13 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-13 div.sk-item::before, #sk-container-id-13 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-13 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-13 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-13 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-13 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-13 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-13 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-13 div.sk-label-container {text-align: center;}#sk-container-id-13 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-13 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-13\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;mlogloss&#x27;,\n",
       "              feature_types=None, gamma=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
       "              n_jobs=None, num_parallel_tree=None, objective=&#x27;multi:softprob&#x27;, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" checked><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;mlogloss&#x27;,\n",
       "              feature_types=None, gamma=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
       "              n_jobs=None, num_parallel_tree=None, objective=&#x27;multi:softprob&#x27;, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric='mlogloss',\n",
       "              feature_types=None, gamma=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
       "              n_jobs=None, num_parallel_tree=None, objective='multi:softprob', ...)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train Decision tree classifier\n",
    "dtc.fit(X_train, y_train)\n",
    "\n",
    "#train Random forest classifier\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "#train Logistic regression classifier\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "#train Support vector machine classifier\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "#train K-nearest neighbors classifier\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "#train XGBoost classifier\n",
    "xgbc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision tree classifier score\n",
    "dtc_score = dtc.score(X_test, y_test)\n",
    "\n",
    "#Random forest classifier score\n",
    "rfc_score = rfc.score(X_test, y_test)\n",
    "\n",
    "#Logistic regression classifier score\n",
    "logreg_score = logreg.score(X_test, y_test)\n",
    "\n",
    "#Support vector machine classifier score\n",
    "svc_score = svc.score(X_test, y_test)\n",
    "\n",
    "#K-nearest neighbors classifier score\n",
    "knn_score = knn.score(X_test, y_test)\n",
    "\n",
    "#XGBoost classifier score\n",
    "xgbc_score = xgbc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Model  Accuracy Score\n",
      "5              XGBoost        0.893452\n",
      "1        Random Forest        0.891787\n",
      "0        Decision Tree        0.849612\n",
      "4                  KNN        0.623751\n",
      "2  Logistic Regression        0.529967\n",
      "3                  SVM        0.490566\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.71      0.72       452\n",
      "           1       0.73      0.76      0.74       466\n",
      "           2       0.98      0.97      0.98       884\n",
      "\n",
      "    accuracy                           0.85      1802\n",
      "   macro avg       0.81      0.81      0.81      1802\n",
      "weighted avg       0.85      0.85      0.85      1802\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Print the scores\n",
    "\n",
    "# Create a dictionary with the model names and their scores\n",
    "model_scores = {\n",
    "    \"Decision Tree\": dtc_score,\n",
    "    \"Random Forest\": rfc_score,\n",
    "    \"Logistic Regression\": logreg_score,\n",
    "    \"SVM\": svc_score,\n",
    "    \"KNN\": knn_score,\n",
    "    \"XGBoost\": xgbc_score\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "scores_df = pd.DataFrame(list(model_scores.items()), columns=['Model', 'Accuracy Score'])\n",
    "\n",
    "# Sort the DataFrame by 'Accuracy Score' in descending order\n",
    "scores_df = scores_df.sort_values(by='Accuracy Score', ascending=False)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(scores_df)\n",
    "\n",
    "base_report = classification_report(y_test, xgbc.predict(X_test))\n",
    "\n",
    "print(f\"\\n{classification_report(y_test, dtc.predict(X_test))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the above scores, we have a benchmark to work with. We also know that the median is 2 for the target variable. And if you chose the median for every prediction you would be right approximately 50% of the time. Looking at the scores from our models, we can now see which models had the greatest gains over simply choosing the median every time. \n",
    "\n",
    "We can also see which models performed worse than our simplest guess. \n",
    "\n",
    "Before we cast judgment on the performance of these models, we have to remember that some are more sensitive to feature scaling than others. For example, Logistic Regression, SVM, and KNN all benefit greatly from feature scaling, while Decision Trees, Random Forest, and XGBoost all are typically invariant to feature scaling. \n",
    "\n",
    "What this tells us is that in order to utilize some of these models properly we should perform some sort of feature scaling on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For SVM and Logistic Regression, we will use the Z-score normalization or `StandardScaler()` from the Scikit Learn library. \n",
    "\n",
    "For KNN, we will use Min-Max scaling (normalization) or the `MinMaxScaler()` also from Scikit Learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the scalers to the train and test sets separately and transform both\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_stndrd = scaler.fit_transform(X_train)\n",
    "X_test_stndrd = scaler.transform(X_test)\n",
    "\n",
    "minmax = MinMaxScaler()\n",
    "\n",
    "X_train_minmax = minmax.fit_transform(X_train)\n",
    "X_test_minmax = minmax.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the scaled data to retrain our models. We can compare it with our tree models without scaled data, as well as pass the scaled data through the tree models to evaluate performance. These scores can act as our new benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-9 {color: black;background-color: white;}#sk-container-id-9 pre{padding: 0;}#sk-container-id-9 div.sk-toggleable {background-color: white;}#sk-container-id-9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-9 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-9 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-9 div.sk-item {position: relative;z-index: 1;}#sk-container-id-9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-9 div.sk-item::before, #sk-container-id-9 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-9 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-9 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-9 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-9 div.sk-label-container {text-align: center;}#sk-container-id-9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-9 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;mlogloss&#x27;,\n",
       "              feature_types=None, gamma=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
       "              n_jobs=None, num_parallel_tree=None, objective=&#x27;multi:softprob&#x27;, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" checked><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;mlogloss&#x27;,\n",
       "              feature_types=None, gamma=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
       "              n_jobs=None, num_parallel_tree=None, objective=&#x27;multi:softprob&#x27;, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric='mlogloss',\n",
       "              feature_types=None, gamma=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
       "              n_jobs=None, num_parallel_tree=None, objective='multi:softprob', ...)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training our tree and ensemble models on the scaled data\n",
    "#train Decision tree classifier\n",
    "dtc.fit(X_train_stndrd, y_train)\n",
    "\n",
    "#train Random forest classifier\n",
    "rfc.fit(X_train_stndrd, y_train)\n",
    "\n",
    "#train XGBoost classifier\n",
    "xgbc.fit(X_train_stndrd, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-10 {color: black;background-color: white;}#sk-container-id-10 pre{padding: 0;}#sk-container-id-10 div.sk-toggleable {background-color: white;}#sk-container-id-10 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-10 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-10 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-10 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-10 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-10 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-10 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-10 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-10 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-10 div.sk-item {position: relative;z-index: 1;}#sk-container-id-10 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-10 div.sk-item::before, #sk-container-id-10 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-10 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-10 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-10 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-10 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-10 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-10 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-10 div.sk-label-container {text-align: center;}#sk-container-id-10 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-10 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-10\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" checked><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training the remaining models on their preferred scaling method\n",
    "#train Logistic regression classifier\n",
    "logreg.fit(X_train_stndrd, y_train)\n",
    "\n",
    "#train Support vector machine classifier\n",
    "svc.fit(X_train_stndrd, y_train)\n",
    "\n",
    "#train K-nearest neighbors classifier\n",
    "knn.fit(X_train_minmax, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision tree classifier score\n",
    "dtc_score = dtc.score(X_test_stndrd, y_test)\n",
    "\n",
    "#Random forest classifier score\n",
    "rfc_score = rfc.score(X_test_stndrd, y_test)\n",
    "\n",
    "#Logistic regression classifier score\n",
    "logreg_score = logreg.score(X_test_stndrd, y_test)\n",
    "\n",
    "#Support vector machine classifier score\n",
    "svc_score = svc.score(X_test_stndrd, y_test)\n",
    "\n",
    "#K-nearest neighbors classifier score\n",
    "knn_score = knn.score(X_test_minmax, y_test)\n",
    "\n",
    "#XGBoost classifier score\n",
    "xgbc_score = xgbc.score(X_test_stndrd, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Model  Accuracy Score\n",
      "5              XGBoost        0.893452\n",
      "1        Random Forest        0.887902\n",
      "0        Decision Tree        0.851276\n",
      "4                  KNN        0.623751\n",
      "2  Logistic Regression        0.529967\n",
      "3                  SVM        0.490566\n",
      "         Model-Scaling  Accuracy Score\n",
      "5              XGBoost        0.893452\n",
      "1        Random Forest        0.892342\n",
      "2  Logistic Regression        0.876249\n",
      "3                  SVM        0.869589\n",
      "0        Decision Tree        0.846837\n",
      "4                  KNN        0.809101\n",
      "\n",
      " Classification Report using StandardScaler: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.71      0.71       452\n",
      "           1       0.72      0.75      0.74       466\n",
      "           2       0.98      0.97      0.97       884\n",
      "\n",
      "    accuracy                           0.85      1802\n",
      "   macro avg       0.81      0.81      0.81      1802\n",
      "weighted avg       0.85      0.85      0.85      1802\n",
      "\n",
      "\n",
      " Classification Report using MinMaxScaler: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.56      0.60       452\n",
      "           1       0.62      0.71      0.66       466\n",
      "           2       0.99      0.99      0.99       884\n",
      "\n",
      "    accuracy                           0.81      1802\n",
      "   macro avg       0.75      0.75      0.75      1802\n",
      "weighted avg       0.81      0.81      0.81      1802\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Print the scores\n",
    "\n",
    "# Create a dictionary with the model names and their scores\n",
    "model_scores_scaled = {\n",
    "    \"Decision Tree\": dtc_score,\n",
    "    \"Random Forest\": rfc_score,\n",
    "    \"Logistic Regression\": logreg_score,\n",
    "    \"SVM\": svc_score,\n",
    "    \"KNN\": knn_score,\n",
    "    \"XGBoost\": xgbc_score\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "scores_scaled = pd.DataFrame(list(model_scores_scaled.items()), columns=['Model-Scaling', 'Accuracy Score'])\n",
    "\n",
    "# Sort the DataFrame by 'Accuracy Score' in descending order\n",
    "scores_scaled = scores_scaled.sort_values(by='Accuracy Score', ascending=False)\n",
    "\n",
    "# Display the DataFrame for both sets of scores\n",
    "print(scores_df)\n",
    "print(scores_scaled)\n",
    "\n",
    "print(f\"\\n Classification Report using StandardScaler: \\n{classification_report(y_test, dtc.predict(X_test_stndrd))}\")\n",
    "print(f\"\\n Classification Report using MinMaxScaler: \\n{classification_report(y_test, knn.predict(X_test_minmax))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see huge improvements in the accuracy just by applying scaling to our data for the the models that are typically sensitive to it. Our ensemble models maintained their performance rankings with only a minor change in the random forest performance and no change in the performance of XGBoost. \n",
    "\n",
    "Logistic Regression had the largest gain of 0.35. Every model is now performing in the 80% territory which is fairly strong when compared to our absolute base benchmark of always choosing the median. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the data cleaning and EDA notebook, we saw that there were potential outliers, not only in the data, but also in the error values. We opted to leave the outliers in due to a lack of domain knowledge. But as part of our preprocessing we can not remove some of those extreme values and compare performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start off with very conservative outlier removal, to see if this improves our performance. Since we saw vast improvements from our scaling, we will also scale the data once outliers have been removed. From what we saw in our data exploration notebook, most of the variables were left skewed, with their most extreme values being greater than the mean. Eliminating outliers outside of IQR + 3 * $\\sigma$ can help remove these extreme values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Z-score for each feature in the DataFrame\n",
    "z_scores = (kois - kois.mean()) / kois.std()\n",
    "\n",
    "# Identify rows where any feature has a Z-score greater than 4\n",
    "outliers = (z_scores > 3)\n",
    "\n",
    "# Filter those rows from the DataFrame\n",
    "kois_no_outliers = kois[~outliers.any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7418, 40)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print the shape of our original data and our new dataset with no outliers\n",
    "kois_no_outliers.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the above that we have eliminated about 1050 values that were categorized as outliers. We can now split our new data set, and apply our feature scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate our target variable from the rest of the data\n",
    "y_no_outliers = kois_no_outliers['koi_disposition_encoded']\n",
    "X_not_outliers = kois_no_outliers.drop(['koi_disposition_encoded'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into training and testing sets\n",
    "#NO suffix is for no outliers\n",
    "X_train_NO, X_test_NO, y_train_NO, y_test_NO = train_test_split(X_not_outliers, y_no_outliers, test_size=0.2, random_state=42) #use a test size of 20%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the data without outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the scalers to the train and test sets separately and transform both\n",
    "\n",
    "#standard scaling\n",
    "X_train_stndrd_NO = scaler.fit_transform(X_train_NO)\n",
    "X_test_stndrd_NO = scaler.transform(X_test_NO)\n",
    "\n",
    "#minmax scaling\n",
    "X_train_minmax_NO = minmax.fit_transform(X_train_NO)\n",
    "X_test_minmax_NO = minmax.transform(X_test_NO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-11 {color: black;background-color: white;}#sk-container-id-11 pre{padding: 0;}#sk-container-id-11 div.sk-toggleable {background-color: white;}#sk-container-id-11 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-11 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-11 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-11 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-11 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-11 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-11 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-11 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-11 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-11 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-11 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-11 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-11 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-11 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-11 div.sk-item {position: relative;z-index: 1;}#sk-container-id-11 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-11 div.sk-item::before, #sk-container-id-11 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-11 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-11 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-11 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-11 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-11 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-11 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-11 div.sk-label-container {text-align: center;}#sk-container-id-11 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-11 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-11\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;mlogloss&#x27;,\n",
       "              feature_types=None, gamma=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
       "              n_jobs=None, num_parallel_tree=None, objective=&#x27;multi:softprob&#x27;, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" checked><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;mlogloss&#x27;,\n",
       "              feature_types=None, gamma=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
       "              n_jobs=None, num_parallel_tree=None, objective=&#x27;multi:softprob&#x27;, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric='mlogloss',\n",
       "              feature_types=None, gamma=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
       "              n_jobs=None, num_parallel_tree=None, objective='multi:softprob', ...)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training our tree and ensemble models on the scaled data with outliers removed\n",
    "#train Decision tree classifier\n",
    "dtc.fit(X_train_stndrd_NO, y_train_NO)\n",
    "\n",
    "#train Random forest classifier\n",
    "rfc.fit(X_train_stndrd_NO, y_train_NO)\n",
    "\n",
    "#train XGBoost classifier\n",
    "xgbc.fit(X_train_minmax_NO, y_train_NO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-12 {color: black;background-color: white;}#sk-container-id-12 pre{padding: 0;}#sk-container-id-12 div.sk-toggleable {background-color: white;}#sk-container-id-12 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-12 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-12 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-12 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-12 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-12 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-12 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-12 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-12 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-12 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-12 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-12 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-12 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-12 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-12 div.sk-item {position: relative;z-index: 1;}#sk-container-id-12 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-12 div.sk-item::before, #sk-container-id-12 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-12 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-12 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-12 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-12 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-12 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-12 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-12 div.sk-label-container {text-align: center;}#sk-container-id-12 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-12 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-12\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" checked><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training the remaining models on their preferred scaling method with outliers removed\n",
    "#train Logistic regression classifier\n",
    "logreg.fit(X_train_stndrd_NO, y_train_NO)\n",
    "\n",
    "#train Support vector machine classifier\n",
    "svc.fit(X_train_stndrd_NO, y_train_NO)\n",
    "\n",
    "#train K-nearest neighbors classifier\n",
    "knn.fit(X_train_stndrd_NO, y_train_NO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance of models with outliers removed, and scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision tree classifier score\n",
    "dtc_score = dtc.score(X_test_stndrd_NO, y_test_NO)\n",
    "\n",
    "#Random forest classifier score\n",
    "rfc_score = rfc.score(X_test_stndrd_NO, y_test_NO)\n",
    "\n",
    "#Logistic regression classifier score\n",
    "logreg_score = logreg.score(X_test_stndrd_NO, y_test_NO)\n",
    "\n",
    "#Support vector machine classifier score\n",
    "svc_score = svc.score(X_test_stndrd_NO, y_test_NO)\n",
    "\n",
    "#K-nearest neighbors classifier score\n",
    "knn_score = knn.score(X_test_minmax_NO, y_test_NO)\n",
    "\n",
    "#XGBoost classifier score\n",
    "xgbc_score = xgbc.score(X_test_stndrd_NO, y_test_NO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Model  Accuracy Score\n",
      "5              XGBoost        0.893452\n",
      "1        Random Forest        0.887902\n",
      "0        Decision Tree        0.851276\n",
      "4                  KNN        0.623751\n",
      "2  Logistic Regression        0.529967\n",
      "3                  SVM        0.490566\n",
      "         Model-Scaling  Accuracy Score\n",
      "5              XGBoost        0.893452\n",
      "1        Random Forest        0.892342\n",
      "2  Logistic Regression        0.876249\n",
      "3                  SVM        0.869589\n",
      "0        Decision Tree        0.846837\n",
      "4                  KNN        0.809101\n",
      "  Model-Scaling-No Outliers  Accuracy Score\n",
      "1             Random Forest        0.884097\n",
      "2       Logistic Regression        0.882075\n",
      "3                       SVM        0.879380\n",
      "0             Decision Tree        0.828841\n",
      "5                   XGBoost        0.671833\n",
      "4                       KNN        0.598383\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.92      0.60       366\n",
      "           1       1.00      0.02      0.03       455\n",
      "           2       0.90      0.99      0.94       663\n",
      "\n",
      "    accuracy                           0.67      1484\n",
      "   macro avg       0.78      0.64      0.52      1484\n",
      "weighted avg       0.82      0.67      0.58      1484\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Print the scores\n",
    "\n",
    "# Create a dictionary with the model names and their scores\n",
    "model_scores_scaled_NO = {\n",
    "    \"Decision Tree\": dtc_score,\n",
    "    \"Random Forest\": rfc_score,\n",
    "    \"Logistic Regression\": logreg_score,\n",
    "    \"SVM\": svc_score,\n",
    "    \"KNN\": knn_score,\n",
    "    \"XGBoost\": xgbc_score\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "scores_scaled_NO = pd.DataFrame(list(model_scores_scaled_NO.items()), columns=['Model-Scaling-No Outliers', 'Accuracy Score'])\n",
    "\n",
    "# Sort the DataFrame by 'Accuracy Score' in descending order\n",
    "scores_scaled_NO = scores_scaled_NO.sort_values(by='Accuracy Score', ascending=False)\n",
    "\n",
    "# Display the DataFrame for both sets of scores\n",
    "print(scores_df)\n",
    "print(scores_scaled)\n",
    "print(scores_scaled_NO)\n",
    "\n",
    "print(classification_report(y_test_NO, xgbc.predict(X_test_stndrd_NO)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw drastic performance loss in some of our models, and even some performance loss in our top performaning models. This indicates that the so-called outliers, can not simply be filtered out using the methods that were employed. We will continue with the data as is, and look at other possibilities for preprocessing that may improve performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logarithmic Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in our data exploration, as well as when viewing the descriptive statistics of our cleaned dataset, there was a very wide range of values for many of the variables. Despite the scaling, these extreme values can still negatively impact our model performance. Another method for dealing with extreme values is log scaling. \n",
    "\n",
    "Log scaling also makes sense in the context of our data as it is measurements on an astronomical scale, where magnitudes can very logarithmically.\n",
    "\n",
    "As there are no built in log-scalers, we can create our own. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import relevant libraries\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "#Create a class to scale the data using the log function from numpy\n",
    "class LogScaler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, epsilon=1e-5):\n",
    "        self.epsilon = epsilon  # Small constant to handle zero values\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    #ned to take the absolute value of the data to avoid errors\n",
    "    def transform(self, X):\n",
    "        return np.log1p(np.abs(X + self.epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply the log scaler to the data\n",
    "log_scaler = LogScaler(epsilon=1e-5)\n",
    "\n",
    "X_train_log = log_scaler.fit_transform(X_train)\n",
    "X_test_log = log_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can train our models without our Standard or MinMax Scalers to compare performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;mlogloss&#x27;,\n",
       "              feature_types=None, gamma=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
       "              n_jobs=None, num_parallel_tree=None, objective=&#x27;multi:softprob&#x27;, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;mlogloss&#x27;,\n",
       "              feature_types=None, gamma=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
       "              n_jobs=None, num_parallel_tree=None, objective=&#x27;multi:softprob&#x27;, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric='mlogloss',\n",
       "              feature_types=None, gamma=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
       "              n_jobs=None, num_parallel_tree=None, objective='multi:softprob', ...)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train Decision tree classifier\n",
    "dtc.fit(X_train_log, y_train)\n",
    "\n",
    "#train Random forest classifier\n",
    "rfc.fit(X_train_log, y_train)\n",
    "\n",
    "#train Logistic regression classifier\n",
    "logreg.fit(X_train_log, y_train)\n",
    "\n",
    "#train Support vector machine classifier\n",
    "svc.fit(X_train_log, y_train)\n",
    "\n",
    "#train K-nearest neighbors classifier\n",
    "knn.fit(X_train_log, y_train)\n",
    "\n",
    "#train XGBoost classifier\n",
    "xgbc.fit(X_train_log, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision tree classifier score\n",
    "dtc_score = dtc.score(X_test_log, y_test)\n",
    "\n",
    "#Random forest classifier score\n",
    "rfc_score = rfc.score(X_test_log, y_test)\n",
    "\n",
    "#Logistic regression classifier score\n",
    "logreg_score = logreg.score(X_test_log, y_test)\n",
    "\n",
    "#Support vector machine classifier score\n",
    "svc_score = svc.score(X_test_log, y_test)\n",
    "\n",
    "#K-nearest neighbors classifier score\n",
    "knn_score = knn.score(X_test_log, y_test)\n",
    "\n",
    "#XGBoost classifier score\n",
    "xgbc_score = xgbc.score(X_test_log, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with our scaled data using normal transformers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Model-Scaling  Accuracy Score\n",
      "5              XGBoost        0.893452\n",
      "1        Random Forest        0.890677\n",
      "2  Logistic Regression        0.876249\n",
      "3                  SVM        0.869589\n",
      "0        Decision Tree        0.847392\n",
      "4                  KNN        0.809101\n",
      "     Model-Log-Scaling  Accuracy Score\n",
      "5              XGBoost        0.896781\n",
      "2  Logistic Regression        0.894562\n",
      "1        Random Forest        0.891232\n",
      "3                  SVM        0.865705\n",
      "0        Decision Tree        0.846282\n",
      "4                  KNN        0.798557\n",
      "\n",
      " Classification Report for XGBoost: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.77      0.79       452\n",
      "           1       0.80      0.83      0.82       466\n",
      "           2       0.99      0.99      0.99       884\n",
      "\n",
      "    accuracy                           0.90      1802\n",
      "   macro avg       0.87      0.87      0.87      1802\n",
      "weighted avg       0.90      0.90      0.90      1802\n",
      "\n",
      "\n",
      " Classification Report for Randome Forest: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.94      0.63       452\n",
      "           1       0.00      0.00      0.00       466\n",
      "           2       0.96      0.99      0.98       884\n",
      "\n",
      "    accuracy                           0.72      1802\n",
      "   macro avg       0.48      0.65      0.54      1802\n",
      "weighted avg       0.59      0.72      0.64      1802\n",
      "\n",
      "\n",
      " Classification Report for Logistic Regression: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.88      0.59       452\n",
      "           1       0.29      0.12      0.17       466\n",
      "           2       0.99      0.80      0.88       884\n",
      "\n",
      "    accuracy                           0.64      1802\n",
      "   macro avg       0.57      0.60      0.55      1802\n",
      "weighted avg       0.67      0.64      0.63      1802\n",
      "\n",
      "\n",
      " Classification Report for SVM: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       452\n",
      "           1       0.00      0.00      0.00       466\n",
      "           2       0.49      1.00      0.66       884\n",
      "\n",
      "    accuracy                           0.49      1802\n",
      "   macro avg       0.16      0.33      0.22      1802\n",
      "weighted avg       0.24      0.49      0.32      1802\n",
      "\n",
      "\n",
      " Classification Report for Decision Tree: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.99      0.65       452\n",
      "           1       0.00      0.00      0.00       466\n",
      "           2       0.99      0.87      0.92       884\n",
      "\n",
      "    accuracy                           0.67      1802\n",
      "   macro avg       0.49      0.62      0.53      1802\n",
      "weighted avg       0.61      0.67      0.62      1802\n",
      "\n",
      "\n",
      " Classification Report for KNN: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       452\n",
      "           1       0.00      0.00      0.00       466\n",
      "           2       0.49      1.00      0.66       884\n",
      "\n",
      "    accuracy                           0.49      1802\n",
      "   macro avg       0.16      0.33      0.22      1802\n",
      "weighted avg       0.24      0.49      0.32      1802\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Print the scores\n",
    "\n",
    "# Create a dictionary with the model names and their scores\n",
    "model_scores_log = {\n",
    "    \"Decision Tree\": dtc_score,\n",
    "    \"Random Forest\": rfc_score,\n",
    "    \"Logistic Regression\": logreg_score,\n",
    "    \"SVM\": svc_score,\n",
    "    \"KNN\": knn_score,\n",
    "    \"XGBoost\": xgbc_score\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "scores_log = pd.DataFrame(list(model_scores_log.items()), columns=['Model-Log-Scaling', 'Accuracy Score'])\n",
    "\n",
    "# Sort the DataFrame by 'Accuracy Score' in descending order\n",
    "scores_log = scores_log.sort_values(by='Accuracy Score', ascending=False)\n",
    "\n",
    "# Display the DataFrame for both sets of scores\n",
    "print(scores_scaled)\n",
    "print(scores_log)\n",
    "\n",
    "print(f\"\\n Classification Report for XGBoost: \\n{classification_report(y_test, xgbc.predict(X_test_log))}\")\n",
    "print(f\"\\n Classification Report for Random Forest: \\n{classification_report(y_test, rfc.predict(X_test_minmax))}\")\n",
    "print(f\"\\n Classification Report for Logistic Regression: \\n{classification_report(y_test, logreg.predict(X_test_stndrd))}\")\n",
    "print(f\"\\n Classification Report for SVM: \\n{classification_report(y_test, svc.predict(X_test_minmax))}\")\n",
    "print(f\"\\n Classification Report for Decision Tree: \\n{classification_report(y_test, dtc.predict(X_test_minmax))}\")\n",
    "print(f\"\\n Classification Report for KNN: \\n{classification_report(y_test, knn.predict(X_test_minmax))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the output from our `classification_report`, we see some obvious issues with the performance of our models. XGBoost out of the box is doing okay, but still favours the majority class. \n",
    "\n",
    "Random Forest had precision, recall and f1-score for 1 (a CONFIRMED exoplanet) of zero. Meaning it may have only ever predicted 0 and 2 for all test values. \n",
    "\n",
    "Logistic Regression has okay scores for class 2 (FALSE POSITIVE), but very poor scores for our CONFIRMED class. \n",
    "\n",
    "These types of patterns of poor performance are similar for the remaining models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a next step, we are going to perorm feature selection to see if we can out do any of the above scores. \n",
    "\n",
    "The most basic feature selection method we can apply, is simply removing columns we suspect may negatively impact model performance. Essentially we can complete this, then compare performance. \n",
    "\n",
    "To start off, we can remove the error columns, which constitute about 2/3rds of the columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9007, 20)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>koi_fpflag_nt</th>\n",
       "      <th>koi_fpflag_ss</th>\n",
       "      <th>koi_fpflag_co</th>\n",
       "      <th>koi_fpflag_ec</th>\n",
       "      <th>koi_period</th>\n",
       "      <th>koi_time0bk</th>\n",
       "      <th>koi_impact</th>\n",
       "      <th>koi_duration</th>\n",
       "      <th>koi_depth</th>\n",
       "      <th>koi_prad</th>\n",
       "      <th>koi_teq</th>\n",
       "      <th>koi_insol</th>\n",
       "      <th>koi_model_snr</th>\n",
       "      <th>koi_steff</th>\n",
       "      <th>koi_slogg</th>\n",
       "      <th>koi_srad</th>\n",
       "      <th>ra</th>\n",
       "      <th>dec</th>\n",
       "      <th>koi_kepmag</th>\n",
       "      <th>koi_disposition_encoded</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rowid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.488036</td>\n",
       "      <td>170.538750</td>\n",
       "      <td>0.146</td>\n",
       "      <td>2.95750</td>\n",
       "      <td>615.8</td>\n",
       "      <td>2.26</td>\n",
       "      <td>793.0</td>\n",
       "      <td>93.59</td>\n",
       "      <td>35.8</td>\n",
       "      <td>5455.0</td>\n",
       "      <td>4.467</td>\n",
       "      <td>0.927</td>\n",
       "      <td>291.93423</td>\n",
       "      <td>48.141651</td>\n",
       "      <td>15.347</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>54.418383</td>\n",
       "      <td>162.513840</td>\n",
       "      <td>0.586</td>\n",
       "      <td>4.50700</td>\n",
       "      <td>874.8</td>\n",
       "      <td>2.83</td>\n",
       "      <td>443.0</td>\n",
       "      <td>9.11</td>\n",
       "      <td>25.8</td>\n",
       "      <td>5455.0</td>\n",
       "      <td>4.467</td>\n",
       "      <td>0.927</td>\n",
       "      <td>291.93423</td>\n",
       "      <td>48.141651</td>\n",
       "      <td>15.347</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19.899140</td>\n",
       "      <td>175.850252</td>\n",
       "      <td>0.969</td>\n",
       "      <td>1.78220</td>\n",
       "      <td>10829.0</td>\n",
       "      <td>14.60</td>\n",
       "      <td>638.0</td>\n",
       "      <td>39.30</td>\n",
       "      <td>76.3</td>\n",
       "      <td>5853.0</td>\n",
       "      <td>4.544</td>\n",
       "      <td>0.868</td>\n",
       "      <td>297.00482</td>\n",
       "      <td>48.134129</td>\n",
       "      <td>15.436</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.736952</td>\n",
       "      <td>170.307565</td>\n",
       "      <td>1.276</td>\n",
       "      <td>2.40641</td>\n",
       "      <td>8079.2</td>\n",
       "      <td>33.46</td>\n",
       "      <td>1395.0</td>\n",
       "      <td>891.96</td>\n",
       "      <td>505.6</td>\n",
       "      <td>5805.0</td>\n",
       "      <td>4.564</td>\n",
       "      <td>0.791</td>\n",
       "      <td>285.53461</td>\n",
       "      <td>48.285210</td>\n",
       "      <td>15.597</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.525592</td>\n",
       "      <td>171.595550</td>\n",
       "      <td>0.701</td>\n",
       "      <td>1.65450</td>\n",
       "      <td>603.3</td>\n",
       "      <td>2.75</td>\n",
       "      <td>1406.0</td>\n",
       "      <td>926.16</td>\n",
       "      <td>40.9</td>\n",
       "      <td>6031.0</td>\n",
       "      <td>4.438</td>\n",
       "      <td>1.046</td>\n",
       "      <td>288.75488</td>\n",
       "      <td>48.226200</td>\n",
       "      <td>15.509</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       koi_fpflag_nt  koi_fpflag_ss  koi_fpflag_co  koi_fpflag_ec  koi_period  \\\n",
       "rowid                                                                           \n",
       "1                  0              0              0              0    9.488036   \n",
       "2                  0              0              0              0   54.418383   \n",
       "3                  0              1              0              0   19.899140   \n",
       "4                  0              1              0              0    1.736952   \n",
       "5                  0              0              0              0    2.525592   \n",
       "\n",
       "       koi_time0bk  koi_impact  koi_duration  koi_depth  koi_prad  koi_teq  \\\n",
       "rowid                                                                        \n",
       "1       170.538750       0.146       2.95750      615.8      2.26    793.0   \n",
       "2       162.513840       0.586       4.50700      874.8      2.83    443.0   \n",
       "3       175.850252       0.969       1.78220    10829.0     14.60    638.0   \n",
       "4       170.307565       1.276       2.40641     8079.2     33.46   1395.0   \n",
       "5       171.595550       0.701       1.65450      603.3      2.75   1406.0   \n",
       "\n",
       "       koi_insol  koi_model_snr  koi_steff  koi_slogg  koi_srad         ra  \\\n",
       "rowid                                                                        \n",
       "1          93.59           35.8     5455.0      4.467     0.927  291.93423   \n",
       "2           9.11           25.8     5455.0      4.467     0.927  291.93423   \n",
       "3          39.30           76.3     5853.0      4.544     0.868  297.00482   \n",
       "4         891.96          505.6     5805.0      4.564     0.791  285.53461   \n",
       "5         926.16           40.9     6031.0      4.438     1.046  288.75488   \n",
       "\n",
       "             dec  koi_kepmag  koi_disposition_encoded  \n",
       "rowid                                                  \n",
       "1      48.141651      15.347                        1  \n",
       "2      48.141651      15.347                        1  \n",
       "3      48.134129      15.436                        2  \n",
       "4      48.285210      15.597                        2  \n",
       "5      48.226200      15.509                        1  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove error columns from kois\n",
    "kois_no_err = kois.loc[:, ~kois.columns.str.contains('_err')]\n",
    "print(kois_no_err.shape)\n",
    "kois_no_err.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate our target variable from the rest of the data\n",
    "y_no_err = kois_no_err['koi_disposition_encoded']\n",
    "X_no_err = kois_no_err.drop(['koi_disposition_encoded'], axis=1)\n",
    "\n",
    "#split the data into training and testing sets\n",
    "X_train_no_err, X_test_no_err, y_train_no_err, y_test_no_err = train_test_split(X_no_err, y_no_err, test_size=0.2, random_state=42) #use a test size of 20%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will stick to XGBoost moving forward as it is has been our best performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Base Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.77      0.79       452\n",
      "           1       0.80      0.82      0.81       466\n",
      "           2       0.99      0.99      0.99       884\n",
      "\n",
      "    accuracy                           0.89      1802\n",
      "   macro avg       0.86      0.86      0.86      1802\n",
      "weighted avg       0.89      0.89      0.89      1802\n",
      "\n",
      "\n",
      " Classification Report for XGBoost: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.79      0.81       452\n",
      "           1       0.81      0.84      0.83       466\n",
      "           2       0.99      0.99      0.99       884\n",
      "\n",
      "    accuracy                           0.90      1802\n",
      "   macro avg       0.88      0.88      0.88      1802\n",
      "weighted avg       0.90      0.90      0.90      1802\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#train XGBoost classifier\n",
    "xgbc.fit(X_train_no_err, y_train_no_err)\n",
    "\n",
    "#XGBoost classifier score\n",
    "xgbc_score = xgbc.score(X_test_no_err, y_test_no_err)\n",
    "\n",
    "#classification report for base model and XGBoost\n",
    "print(f\"\\n Base Classification Report: \\n{base_report}\")\n",
    "print(f\"\\n Classification Report for XGBoost: \\n{classification_report(y_test_no_err, xgbc.predict(X_test_no_err))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see when comparing to the base report, that we see some slight performance improvements by dropping the error columns. \n",
    "\n",
    "We can apply a couple more methods before we make a decision as to the best approach for feature selection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7205, 15)\n"
     ]
    }
   ],
   "source": [
    "#import feature selection tools\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "#apply feature selection to the data\n",
    "fs = SelectKBest(score_func=f_classif, k=15)\n",
    "\n",
    "#fit the feature selector to the training data\n",
    "X_selected = fs.fit_transform(X_train, y_train)\n",
    "#transform the test data\n",
    "X_test_selected = fs.transform(X_test)\n",
    "\n",
    "#check the shape of the selected data\n",
    "print(X_selected.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Base Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.77      0.79       452\n",
      "           1       0.80      0.82      0.81       466\n",
      "           2       0.99      0.99      0.99       884\n",
      "\n",
      "    accuracy                           0.89      1802\n",
      "   macro avg       0.86      0.86      0.86      1802\n",
      "weighted avg       0.89      0.89      0.89      1802\n",
      "\n",
      "\n",
      " Classification Report for XGBoost: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.79      0.80       452\n",
      "           1       0.81      0.82      0.81       466\n",
      "           2       0.99      0.99      0.99       884\n",
      "\n",
      "    accuracy                           0.90      1802\n",
      "   macro avg       0.87      0.87      0.87      1802\n",
      "weighted avg       0.90      0.90      0.90      1802\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#train the XGBoost model using the subset of features\n",
    "xgbc.fit(X_selected, y_train)\n",
    "\n",
    "#XGBoost classifier score\n",
    "xgbc_score_kbest = xgbc.score(X_test_selected, y_test)\n",
    "\n",
    "#classification report for base model and XGBoost\n",
    "xgboost_clf_report = classification_report(y_test, xgbc.predict(X_test_selected))\n",
    "\n",
    "print(f\"\\n Base Classification Report: \\n{base_report}\")\n",
    "print(f\"\\n Classification Report for XGBoost: \\n{xgboost_clf_report}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lhl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
